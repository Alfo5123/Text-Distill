# Text-Distill

We apply the algorithm described in the recent paper ["Dataset Distillation"](https://arxiv.org/pdf/1811.10959.pdf) to non-image datasets as suggested within the [YerevaNN list of open projects](https://mlevn.org/projects/ideas/).

## Table of content
- [Authors](#authors)
- [References](#references)

## Authors

 - [Alfredo de la Fuente](https://alfo5123.github.io/)
 - [Marina Gomtsyan](https://github.com/marinagomtsian)
 
 ## References 
 
 Papers:
 
- **[Dataset Distillation](https://arxiv.org/pdf/1811.10959.pdf)**
- [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://arxiv.org/pdf/1804.07461.pdf)
- [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)

 
 Repositories:
 
 - **[Author's Code](https://github.com/SsnL/dataset-distillation)**
 - [Deep NLP Models on Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch)
 - [Deep NLP Course at ABBYY](https://github.com/DanAnastasyev/DeepNLP-Course)
 
 Datasets :
 - [The Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html)
